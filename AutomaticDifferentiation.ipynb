{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutomaticDifferentiation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO0zrZkiusmvzp1JcHzTttT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dloHqnGvsfwv"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kuc-dEGBJc15"
      },
      "source": [
        "To differentiate automatically, TensorFlow needs to remember what operations happen in what order during the forward pass. Then, during the backward pass, TensorFlow traverses this list of operations in reverse order to compute gradients.\r\n",
        "\r\n",
        "[Gradient tapes](https://www.tensorflow.org/api_docs/python/tf/GradientTape)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2TqPd5qKj08",
        "outputId": "55f6a8a6-c705-44a5-fce4-969a55277761"
      },
      "source": [
        "# Gradient tapes\r\n",
        "\r\n",
        "w = tf.Variable(tf.random.normal((3, 2)), name='w')\r\n",
        "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\r\n",
        "x = [[1., 2., 3.]]\r\n",
        "\r\n",
        "with tf.GradientTape(persistent=True) as tape:\r\n",
        "  y = x @ w + b\r\n",
        "  loss = tf.reduce_mean(y**2)\r\n",
        "\r\n",
        "[dl_dw, dl_db] = tape.gradient(loss, [w, b])\r\n",
        "\r\n",
        "# The gradient w.r.t. each source has the shape of the source:\r\n",
        "print(w.shape)\r\n",
        "print(dl_dw.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 2)\n",
            "(3, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM3LP-q9SBsV"
      },
      "source": [
        "Gradients w.r.t. a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZOwUGg1SD1L",
        "outputId": "2c8b9e80-d078-411c-e5df-eedb1217b1f1"
      },
      "source": [
        "layer = tf.keras.layers.Dense(2, activation='relu')\r\n",
        "x = tf.constant([[1., 2., 3.]])\r\n",
        "\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "  # Forward pass\r\n",
        "  y = layer(x)\r\n",
        "  loss = tf.reduce_mean(y**2)\r\n",
        "\r\n",
        "# Calculate gradients with respect to every trainable variable\r\n",
        "grad = tape.gradient(loss, layer.trainable_variables)\r\n",
        "\r\n",
        "for var, g in zip(layer.trainable_variables, grad):\r\n",
        "  print(f'{var.name}, shape: {g.shape}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dense/kernel:0, shape: (3, 2)\n",
            "dense/bias:0, shape: (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_iwVFfVyGNj"
      },
      "source": [
        "Controlling what the tape watches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAOkaVnnyGws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c367a044-3b44-4404-a5dc-dcf7ba155666"
      },
      "source": [
        "# You can list the variables being watched by the tape using the GradientTape.watched_variables method:\r\n",
        "x = tf.constant(3.0)\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "  tape.watch(x)\r\n",
        "  y = x**2\r\n",
        "# dy = 2x * dx\r\n",
        "dy_dx = tape.gradient(y, x)\r\n",
        "print(dy_dx.numpy())\r\n",
        "\r\n",
        "# A trainable variable\r\n",
        "x0 = tf.Variable(3.0, name='x0')\r\n",
        "# Not trainable\r\n",
        "x1 = tf.Variable(3.0, name='x1', trainable=False)\r\n",
        "# Not a Variable: A variable + tensor returns a tensor.\r\n",
        "x2 = tf.Variable(2.0, name='x2') + 1.0\r\n",
        "# Not a variable\r\n",
        "x3 = tf.constant(3.0, name='x3')\r\n",
        "\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "  y = (x0**2) + (x1**2) + (x2**2)\r\n",
        "\r\n",
        "grad = tape.gradient(y, [x0, x1, x2, x3])\r\n",
        "\r\n",
        "for g in grad:\r\n",
        "  print(g)\r\n",
        "print([var.name for var in tape.watched_variables()])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.0\n",
            "tf.Tensor(6.0, shape=(), dtype=float32)\n",
            "None\n",
            "None\n",
            "None\n",
            "['x0:0']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEwHKFYp84im",
        "outputId": "d587b129-2cf5-4713-c501-0b8d053ca721"
      },
      "source": [
        "x0 = tf.Variable(0.0)\r\n",
        "x1 = tf.Variable(10.0)\r\n",
        "\r\n",
        "with tf.GradientTape(watch_accessed_variables=False) as tape:\r\n",
        "  tape.watch(x1)\r\n",
        "  y0 = tf.math.sin(x0)\r\n",
        "  y1 = tf.nn.softplus(x1)\r\n",
        "  y = y0 + y1\r\n",
        "  ys = tf.reduce_sum(y)\r\n",
        "# Since GradientTape.watch was not called on x0, no gradient is computed with respect to it:\r\n",
        "# dy = 2x * dx\r\n",
        "grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\r\n",
        "\r\n",
        "print('dy/dx0:', grad['x0'])\r\n",
        "print('dy/dx1:', grad['x1'].numpy())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dy/dx0: None\n",
            "dy/dx1: 0.9999546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCEWt5pU9RmT"
      },
      "source": [
        "**Intermediate results**\r\n",
        "\r\n",
        "You can also request gradients of the output with respect to intermediate values computed inside the tf.GradientTape context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuqDz5K-9RBm",
        "outputId": "49494195-b6b0-4f55-a46c-51ae5a1ba65c"
      },
      "source": [
        "x = tf.constant(3.0)\r\n",
        "\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "  tape.watch(x)\r\n",
        "  y = x * x\r\n",
        "  z = y * y\r\n",
        "\r\n",
        "# Use the tape to compute the gradient of z with respect to the\r\n",
        "# intermediate value y.\r\n",
        "# dz_dx = 2 * y, where y = x ** 2\r\n",
        "print(tape.gradient(z, y).numpy())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0TvBXM_9bcS"
      },
      "source": [
        "By default, the resources held by a GradientTape are released as soon as GradientTape.gradient() method is called. To compute multiple gradients over the same computation, create a persistent gradient tape. This allows multiple calls to the gradient() method as resources are released when the tape object is garbage collected. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeJotB0N9dJA",
        "outputId": "fcb50f0c-9b06-41f0-b87e-3904ba9f9a27"
      },
      "source": [
        "x = tf.constant([1, 3.0])\r\n",
        "with tf.GradientTape(persistent=True) as tape:\r\n",
        "  tape.watch(x)\r\n",
        "  y = x * x\r\n",
        "  z = y * y\r\n",
        "\r\n",
        "print(tape.gradient(z, x).numpy())  # 108.0 (4 * x**3 at x = 3)\r\n",
        "print(tape.gradient(y, x).numpy())  # 6.0 (2 * x)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  4. 108.]\n",
            "[2. 6.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJZsvRgTEj8b"
      },
      "source": [
        "Notes on performance\r\n",
        "\r\n",
        "ref:https://www.tensorflow.org/guide/autodiff\r\n",
        "\r\n",
        "There is a tiny overhead associated with doing operations inside a gradient tape context. For most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required.\r\n",
        "\r\n",
        "Gradient tapes use memory to store intermediate results, including inputs and outputs, for use during the backwards pass.\r\n",
        "\r\n",
        "For efficiency, some ops (like ReLU) don't need to keep their intermediate results and they are pruned during the forward pass. However, if you use persistent=True on your tape, nothing is discarded and your peak memory usage will be higher."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2is6DF9FYkg"
      },
      "source": [
        "**Issue U May Encounter: Getting a gradient of None**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPXXiS_YFi1s",
        "outputId": "fecd436c-2718-4e16-a3bf-da4dd4cb3816"
      },
      "source": [
        "# When a target is not connected to a source you will get a gradient of None.\r\n",
        "x = tf.Variable(2.)\r\n",
        "y = tf.Variable(3.)\r\n",
        "\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "  z = y * y\r\n",
        "print(tape.gradient(z, x))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9XAVuTlFoEa"
      },
      "source": [
        "1. Replaced a variable with a tensor.\r\n",
        "\r\n",
        "In the section on \"controlling what the tape watches\" you saw that the tape will automatically watch a tf.Variable but not a tf.Tensor.\r\n",
        "\r\n",
        "One common error is to inadvertently replace a tf.Variable with a tf.Tensor, instead of using Variable.assign to update the tf.Variable. Here is an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxTeIpQTFl08",
        "outputId": "9fc98ff5-93ef-44fd-81f9-99162d965a27"
      },
      "source": [
        "x = tf.Variable(2.0)\r\n",
        "\r\n",
        "for epoch in range(2):\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    y = x+1\r\n",
        "\r\n",
        "  print(type(x).__name__, \":\", tape.gradient(y, x))\r\n",
        "  x = x + 1   # This should be `x.assign_add(1)`"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\n",
            "EagerTensor : None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XdZyuWsFx6U"
      },
      "source": [
        "2. Did calculations outside of TensorFlow\r\n",
        "\r\n",
        "The tape can't record the gradient path if the calculation exits TensorFlow. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvdGYXR_F0br",
        "outputId": "5129d8e1-97bd-45ca-da83-6b597fd77148"
      },
      "source": [
        "x = tf.Variable([[1.0, 2.0],\r\n",
        "                 [3.0, 4.0]], dtype=tf.float32)\r\n",
        "\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "  x2 = x**2\r\n",
        "\r\n",
        "  # This step is calculated with NumPy\r\n",
        "  y = np.mean(x2, axis=0)\r\n",
        "\r\n",
        "  # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\r\n",
        "  # using `tf.convert_to_tensor`.\r\n",
        "  y = tf.reduce_mean(y, axis=0)\r\n",
        "\r\n",
        "print(tape.gradient(y, x))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M2qTRDtF3us"
      },
      "source": [
        "3. Took gradients through an integer or string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isQcHZW7F_hq",
        "outputId": "0f8db0d7-ee35-4d35-fdfb-6a42d8d5bffe"
      },
      "source": [
        "# The x0 variable has an `int` dtype.\r\n",
        "x = tf.Variable([[2, 2],\r\n",
        "                 [2, 2]])\r\n",
        "\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "  # The path to x1 is blocked by the `int` dtype here.\r\n",
        "  y = tf.cast(x, tf.float32)\r\n",
        "  y = tf.reduce_sum(x)\r\n",
        "\r\n",
        "print(tape.gradient(y, x))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
            "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}